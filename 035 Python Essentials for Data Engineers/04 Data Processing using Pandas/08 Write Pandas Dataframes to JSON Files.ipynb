{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_column_names(schemas, ds_name, sorting_key='column_position'):\n",
    "    column_details = schemas[ds_name]\n",
    "    columns = sorted(column_details, key=lambda col: col[sorting_key])\n",
    "    return [col['column_name'] for col in columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "schemas = json.load(open('data/retail_db/schemas.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_columns = get_column_names(schemas, 'orders')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders = pd.read_csv(\n",
    "    'data/retail_db/orders/part-00000',\n",
    "    names=orders_columns\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mSignature:\u001b[0m\n",
      "\u001b[0morders\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mpath_or_buf\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'FilePath | WriteBuffer[bytes] | WriteBuffer[str] | None'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0msep\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'str'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m','\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mna_rep\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'str'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mfloat_format\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'str | Callable | None'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mcolumns\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'Sequence[Hashable] | None'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mheader\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'bool_t | list[str]'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mindex\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'bool_t'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mindex_label\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'IndexLabel | None'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'str'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mencoding\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'str | None'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mcompression\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'CompressionOptions'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'infer'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mquoting\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'int | None'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mquotechar\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'str'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'\"'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mlineterminator\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'str | None'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mchunksize\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'int | None'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mdate_format\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'str | None'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mdoublequote\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'bool_t'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mescapechar\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'str | None'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mdecimal\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'str'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'.'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0merrors\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'str'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'strict'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mstorage_options\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'StorageOptions'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;34m'str | None'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mDocstring:\u001b[0m\n",
      "Write object to a comma-separated values (csv) file.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "path_or_buf : str, path object, file-like object, or None, default None\n",
      "    String, path object (implementing os.PathLike[str]), or file-like\n",
      "    object implementing a write() function. If None, the result is\n",
      "    returned as a string. If a non-binary file object is passed, it should\n",
      "    be opened with `newline=''`, disabling universal newlines. If a binary\n",
      "    file object is passed, `mode` might need to contain a `'b'`.\n",
      "\n",
      "    .. versionchanged:: 1.2.0\n",
      "\n",
      "       Support for binary file objects was introduced.\n",
      "\n",
      "sep : str, default ','\n",
      "    String of length 1. Field delimiter for the output file.\n",
      "na_rep : str, default ''\n",
      "    Missing data representation.\n",
      "float_format : str, Callable, default None\n",
      "    Format string for floating point numbers. If a Callable is given, it takes\n",
      "    precedence over other numeric formatting parameters, like decimal.\n",
      "columns : sequence, optional\n",
      "    Columns to write.\n",
      "header : bool or list of str, default True\n",
      "    Write out the column names. If a list of strings is given it is\n",
      "    assumed to be aliases for the column names.\n",
      "index : bool, default True\n",
      "    Write row names (index).\n",
      "index_label : str or sequence, or False, default None\n",
      "    Column label for index column(s) if desired. If None is given, and\n",
      "    `header` and `index` are True, then the index names are used. A\n",
      "    sequence should be given if the object uses MultiIndex. If\n",
      "    False do not print fields for index names. Use index_label=False\n",
      "    for easier importing in R.\n",
      "mode : str, default 'w'\n",
      "    Python write mode. The available write modes are the same as\n",
      "    :py:func:`open`.\n",
      "encoding : str, optional\n",
      "    A string representing the encoding to use in the output file,\n",
      "    defaults to 'utf-8'. `encoding` is not supported if `path_or_buf`\n",
      "    is a non-binary file object.\n",
      "compression : str or dict, default 'infer'\n",
      "    For on-the-fly compression of the output data. If 'infer' and 'path_or_buf' is\n",
      "    path-like, then detect compression from the following extensions: '.gz',\n",
      "    '.bz2', '.zip', '.xz', '.zst', '.tar', '.tar.gz', '.tar.xz' or '.tar.bz2'\n",
      "    (otherwise no compression).\n",
      "    Set to ``None`` for no compression.\n",
      "    Can also be a dict with key ``'method'`` set\n",
      "    to one of {``'zip'``, ``'gzip'``, ``'bz2'``, ``'zstd'``, ``'tar'``} and other\n",
      "    key-value pairs are forwarded to\n",
      "    ``zipfile.ZipFile``, ``gzip.GzipFile``,\n",
      "    ``bz2.BZ2File``, ``zstandard.ZstdCompressor`` or\n",
      "    ``tarfile.TarFile``, respectively.\n",
      "    As an example, the following could be passed for faster compression and to create\n",
      "    a reproducible gzip archive:\n",
      "    ``compression={'method': 'gzip', 'compresslevel': 1, 'mtime': 1}``.\n",
      "\n",
      "        .. versionadded:: 1.5.0\n",
      "            Added support for `.tar` files.\n",
      "\n",
      "    .. versionchanged:: 1.0.0\n",
      "\n",
      "       May now be a dict with key 'method' as compression mode\n",
      "       and other entries as additional compression options if\n",
      "       compression mode is 'zip'.\n",
      "\n",
      "    .. versionchanged:: 1.1.0\n",
      "\n",
      "       Passing compression options as keys in dict is\n",
      "       supported for compression modes 'gzip', 'bz2', 'zstd', and 'zip'.\n",
      "\n",
      "    .. versionchanged:: 1.2.0\n",
      "\n",
      "        Compression is supported for binary file objects.\n",
      "\n",
      "    .. versionchanged:: 1.2.0\n",
      "\n",
      "        Previous versions forwarded dict entries for 'gzip' to\n",
      "        `gzip.open` instead of `gzip.GzipFile` which prevented\n",
      "        setting `mtime`.\n",
      "\n",
      "quoting : optional constant from csv module\n",
      "    Defaults to csv.QUOTE_MINIMAL. If you have set a `float_format`\n",
      "    then floats are converted to strings and thus csv.QUOTE_NONNUMERIC\n",
      "    will treat them as non-numeric.\n",
      "quotechar : str, default '\\\"'\n",
      "    String of length 1. Character used to quote fields.\n",
      "lineterminator : str, optional\n",
      "    The newline character or character sequence to use in the output\n",
      "    file. Defaults to `os.linesep`, which depends on the OS in which\n",
      "    this method is called ('\\\\n' for linux, '\\\\r\\\\n' for Windows, i.e.).\n",
      "\n",
      "    .. versionchanged:: 1.5.0\n",
      "\n",
      "        Previously was line_terminator, changed for consistency with\n",
      "        read_csv and the standard library 'csv' module.\n",
      "\n",
      "chunksize : int or None\n",
      "    Rows to write at a time.\n",
      "date_format : str, default None\n",
      "    Format string for datetime objects.\n",
      "doublequote : bool, default True\n",
      "    Control quoting of `quotechar` inside a field.\n",
      "escapechar : str, default None\n",
      "    String of length 1. Character used to escape `sep` and `quotechar`\n",
      "    when appropriate.\n",
      "decimal : str, default '.'\n",
      "    Character recognized as decimal separator. E.g. use ',' for\n",
      "    European data.\n",
      "errors : str, default 'strict'\n",
      "    Specifies how encoding and decoding errors are to be handled.\n",
      "    See the errors argument for :func:`open` for a full list\n",
      "    of options.\n",
      "\n",
      "    .. versionadded:: 1.1.0\n",
      "\n",
      "storage_options : dict, optional\n",
      "    Extra options that make sense for a particular storage connection, e.g.\n",
      "    host, port, username, password, etc. For HTTP(S) URLs the key-value pairs\n",
      "    are forwarded to ``urllib.request.Request`` as header options. For other\n",
      "    URLs (e.g. starting with \"s3://\", and \"gcs://\") the key-value pairs are\n",
      "    forwarded to ``fsspec.open``. Please see ``fsspec`` and ``urllib`` for more\n",
      "    details, and for more examples on storage options refer `here\n",
      "    <https://pandas.pydata.org/docs/user_guide/io.html?\n",
      "    highlight=storage_options#reading-writing-remote-files>`_.\n",
      "\n",
      "    .. versionadded:: 1.2.0\n",
      "\n",
      "Returns\n",
      "-------\n",
      "None or str\n",
      "    If path_or_buf is None, returns the resulting csv format as a\n",
      "    string. Otherwise returns None.\n",
      "\n",
      "See Also\n",
      "--------\n",
      "read_csv : Load a CSV file into a DataFrame.\n",
      "to_excel : Write DataFrame to an Excel file.\n",
      "\n",
      "Examples\n",
      "--------\n",
      ">>> df = pd.DataFrame({'name': ['Raphael', 'Donatello'],\n",
      "...                    'mask': ['red', 'purple'],\n",
      "...                    'weapon': ['sai', 'bo staff']})\n",
      ">>> df.to_csv(index=False)\n",
      "'name,mask,weapon\\nRaphael,red,sai\\nDonatello,purple,bo staff\\n'\n",
      "\n",
      "Create 'out.zip' containing 'out.csv'\n",
      "\n",
      ">>> compression_opts = dict(method='zip',\n",
      "...                         archive_name='out.csv')  # doctest: +SKIP\n",
      ">>> df.to_csv('out.zip', index=False,\n",
      "...           compression=compression_opts)  # doctest: +SKIP\n",
      "\n",
      "To write a csv file to a new folder or nested folder you will first\n",
      "need to create it using either Pathlib or os:\n",
      "\n",
      ">>> from pathlib import Path  # doctest: +SKIP\n",
      ">>> filepath = Path('folder/subfolder/out.csv')  # doctest: +SKIP\n",
      ">>> filepath.parent.mkdir(parents=True, exist_ok=True)  # doctest: +SKIP\n",
      ">>> df.to_csv(filepath)  # doctest: +SKIP\n",
      "\n",
      ">>> import os  # doctest: +SKIP\n",
      ">>> os.makedirs('folder/subfolder', exist_ok=True)  # doctest: +SKIP\n",
      ">>> df.to_csv('folder/subfolder/out.csv')  # doctest: +SKIP\n",
      "\u001b[1;31mFile:\u001b[0m      c:\\users\\dgadiraju\\projects\\python-revision-for-de\\pr-venv\\lib\\site-packages\\pandas\\core\\generic.py\n",
      "\u001b[1;31mType:\u001b[0m      method\n"
     ]
    }
   ],
   "source": [
    "orders.to_csv?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs('data/retail_db/orders_json', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mSignature:\u001b[0m\n",
      "\u001b[0morders\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_json\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mpath_or_buf\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'FilePath | WriteBuffer[bytes] | WriteBuffer[str] | None'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0morient\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'str | None'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mdate_format\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'str | None'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mdouble_precision\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'int'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mforce_ascii\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'bool_t'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mdate_unit\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'str'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'ms'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mdefault_handler\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'Callable[[Any], JSONSerializable] | None'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mlines\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'bool_t'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mcompression\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'CompressionOptions'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'infer'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mindex\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'bool_t'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mindent\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'int | None'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mstorage_options\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'StorageOptions'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;34m'str | None'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mDocstring:\u001b[0m\n",
      "Convert the object to a JSON string.\n",
      "\n",
      "Note NaN's and None will be converted to null and datetime objects\n",
      "will be converted to UNIX timestamps.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "path_or_buf : str, path object, file-like object, or None, default None\n",
      "    String, path object (implementing os.PathLike[str]), or file-like\n",
      "    object implementing a write() function. If None, the result is\n",
      "    returned as a string.\n",
      "orient : str\n",
      "    Indication of expected JSON string format.\n",
      "\n",
      "    * Series:\n",
      "\n",
      "        - default is 'index'\n",
      "        - allowed values are: {'split', 'records', 'index', 'table'}.\n",
      "\n",
      "    * DataFrame:\n",
      "\n",
      "        - default is 'columns'\n",
      "        - allowed values are: {'split', 'records', 'index', 'columns',\n",
      "          'values', 'table'}.\n",
      "\n",
      "    * The format of the JSON string:\n",
      "\n",
      "        - 'split' : dict like {'index' -> [index], 'columns' -> [columns],\n",
      "          'data' -> [values]}\n",
      "        - 'records' : list like [{column -> value}, ... , {column -> value}]\n",
      "        - 'index' : dict like {index -> {column -> value}}\n",
      "        - 'columns' : dict like {column -> {index -> value}}\n",
      "        - 'values' : just the values array\n",
      "        - 'table' : dict like {'schema': {schema}, 'data': {data}}\n",
      "\n",
      "        Describing the data, where data component is like ``orient='records'``.\n",
      "\n",
      "date_format : {None, 'epoch', 'iso'}\n",
      "    Type of date conversion. 'epoch' = epoch milliseconds,\n",
      "    'iso' = ISO8601. The default depends on the `orient`. For\n",
      "    ``orient='table'``, the default is 'iso'. For all other orients,\n",
      "    the default is 'epoch'.\n",
      "double_precision : int, default 10\n",
      "    The number of decimal places to use when encoding\n",
      "    floating point values.\n",
      "force_ascii : bool, default True\n",
      "    Force encoded string to be ASCII.\n",
      "date_unit : str, default 'ms' (milliseconds)\n",
      "    The time unit to encode to, governs timestamp and ISO8601\n",
      "    precision.  One of 's', 'ms', 'us', 'ns' for second, millisecond,\n",
      "    microsecond, and nanosecond respectively.\n",
      "default_handler : callable, default None\n",
      "    Handler to call if object cannot otherwise be converted to a\n",
      "    suitable format for JSON. Should receive a single argument which is\n",
      "    the object to convert and return a serialisable object.\n",
      "lines : bool, default False\n",
      "    If 'orient' is 'records' write out line-delimited json format. Will\n",
      "    throw ValueError if incorrect 'orient' since others are not\n",
      "    list-like.\n",
      "compression : str or dict, default 'infer'\n",
      "    For on-the-fly compression of the output data. If 'infer' and 'path_or_buf' is\n",
      "    path-like, then detect compression from the following extensions: '.gz',\n",
      "    '.bz2', '.zip', '.xz', '.zst', '.tar', '.tar.gz', '.tar.xz' or '.tar.bz2'\n",
      "    (otherwise no compression).\n",
      "    Set to ``None`` for no compression.\n",
      "    Can also be a dict with key ``'method'`` set\n",
      "    to one of {``'zip'``, ``'gzip'``, ``'bz2'``, ``'zstd'``, ``'tar'``} and other\n",
      "    key-value pairs are forwarded to\n",
      "    ``zipfile.ZipFile``, ``gzip.GzipFile``,\n",
      "    ``bz2.BZ2File``, ``zstandard.ZstdCompressor`` or\n",
      "    ``tarfile.TarFile``, respectively.\n",
      "    As an example, the following could be passed for faster compression and to create\n",
      "    a reproducible gzip archive:\n",
      "    ``compression={'method': 'gzip', 'compresslevel': 1, 'mtime': 1}``.\n",
      "\n",
      "        .. versionadded:: 1.5.0\n",
      "            Added support for `.tar` files.\n",
      "\n",
      "    .. versionchanged:: 1.4.0 Zstandard support.\n",
      "\n",
      "index : bool, default True\n",
      "    Whether to include the index values in the JSON string. Not\n",
      "    including the index (``index=False``) is only supported when\n",
      "    orient is 'split' or 'table'.\n",
      "indent : int, optional\n",
      "   Length of whitespace used to indent each record.\n",
      "\n",
      "   .. versionadded:: 1.0.0\n",
      "\n",
      "storage_options : dict, optional\n",
      "    Extra options that make sense for a particular storage connection, e.g.\n",
      "    host, port, username, password, etc. For HTTP(S) URLs the key-value pairs\n",
      "    are forwarded to ``urllib.request.Request`` as header options. For other\n",
      "    URLs (e.g. starting with \"s3://\", and \"gcs://\") the key-value pairs are\n",
      "    forwarded to ``fsspec.open``. Please see ``fsspec`` and ``urllib`` for more\n",
      "    details, and for more examples on storage options refer `here\n",
      "    <https://pandas.pydata.org/docs/user_guide/io.html?\n",
      "    highlight=storage_options#reading-writing-remote-files>`_.\n",
      "\n",
      "    .. versionadded:: 1.2.0\n",
      "\n",
      "Returns\n",
      "-------\n",
      "None or str\n",
      "    If path_or_buf is None, returns the resulting json format as a\n",
      "    string. Otherwise returns None.\n",
      "\n",
      "See Also\n",
      "--------\n",
      "read_json : Convert a JSON string to pandas object.\n",
      "\n",
      "Notes\n",
      "-----\n",
      "The behavior of ``indent=0`` varies from the stdlib, which does not\n",
      "indent the output but does insert newlines. Currently, ``indent=0``\n",
      "and the default ``indent=None`` are equivalent in pandas, though this\n",
      "may change in a future release.\n",
      "\n",
      "``orient='table'`` contains a 'pandas_version' field under 'schema'.\n",
      "This stores the version of `pandas` used in the latest revision of the\n",
      "schema.\n",
      "\n",
      "Examples\n",
      "--------\n",
      ">>> import json\n",
      ">>> df = pd.DataFrame(\n",
      "...     [[\"a\", \"b\"], [\"c\", \"d\"]],\n",
      "...     index=[\"row 1\", \"row 2\"],\n",
      "...     columns=[\"col 1\", \"col 2\"],\n",
      "... )\n",
      "\n",
      ">>> result = df.to_json(orient=\"split\")\n",
      ">>> parsed = json.loads(result)\n",
      ">>> json.dumps(parsed, indent=4)  # doctest: +SKIP\n",
      "{\n",
      "    \"columns\": [\n",
      "        \"col 1\",\n",
      "        \"col 2\"\n",
      "    ],\n",
      "    \"index\": [\n",
      "        \"row 1\",\n",
      "        \"row 2\"\n",
      "    ],\n",
      "    \"data\": [\n",
      "        [\n",
      "            \"a\",\n",
      "            \"b\"\n",
      "        ],\n",
      "        [\n",
      "            \"c\",\n",
      "            \"d\"\n",
      "        ]\n",
      "    ]\n",
      "}\n",
      "\n",
      "Encoding/decoding a Dataframe using ``'records'`` formatted JSON.\n",
      "Note that index labels are not preserved with this encoding.\n",
      "\n",
      ">>> result = df.to_json(orient=\"records\")\n",
      ">>> parsed = json.loads(result)\n",
      ">>> json.dumps(parsed, indent=4)  # doctest: +SKIP\n",
      "[\n",
      "    {\n",
      "        \"col 1\": \"a\",\n",
      "        \"col 2\": \"b\"\n",
      "    },\n",
      "    {\n",
      "        \"col 1\": \"c\",\n",
      "        \"col 2\": \"d\"\n",
      "    }\n",
      "]\n",
      "\n",
      "Encoding/decoding a Dataframe using ``'index'`` formatted JSON:\n",
      "\n",
      ">>> result = df.to_json(orient=\"index\")\n",
      ">>> parsed = json.loads(result)\n",
      ">>> json.dumps(parsed, indent=4)  # doctest: +SKIP\n",
      "{\n",
      "    \"row 1\": {\n",
      "        \"col 1\": \"a\",\n",
      "        \"col 2\": \"b\"\n",
      "    },\n",
      "    \"row 2\": {\n",
      "        \"col 1\": \"c\",\n",
      "        \"col 2\": \"d\"\n",
      "    }\n",
      "}\n",
      "\n",
      "Encoding/decoding a Dataframe using ``'columns'`` formatted JSON:\n",
      "\n",
      ">>> result = df.to_json(orient=\"columns\")\n",
      ">>> parsed = json.loads(result)\n",
      ">>> json.dumps(parsed, indent=4)  # doctest: +SKIP\n",
      "{\n",
      "    \"col 1\": {\n",
      "        \"row 1\": \"a\",\n",
      "        \"row 2\": \"c\"\n",
      "    },\n",
      "    \"col 2\": {\n",
      "        \"row 1\": \"b\",\n",
      "        \"row 2\": \"d\"\n",
      "    }\n",
      "}\n",
      "\n",
      "Encoding/decoding a Dataframe using ``'values'`` formatted JSON:\n",
      "\n",
      ">>> result = df.to_json(orient=\"values\")\n",
      ">>> parsed = json.loads(result)\n",
      ">>> json.dumps(parsed, indent=4)  # doctest: +SKIP\n",
      "[\n",
      "    [\n",
      "        \"a\",\n",
      "        \"b\"\n",
      "    ],\n",
      "    [\n",
      "        \"c\",\n",
      "        \"d\"\n",
      "    ]\n",
      "]\n",
      "\n",
      "Encoding with Table Schema:\n",
      "\n",
      ">>> result = df.to_json(orient=\"table\")\n",
      ">>> parsed = json.loads(result)\n",
      ">>> json.dumps(parsed, indent=4)  # doctest: +SKIP\n",
      "{\n",
      "    \"schema\": {\n",
      "        \"fields\": [\n",
      "            {\n",
      "                \"name\": \"index\",\n",
      "                \"type\": \"string\"\n",
      "            },\n",
      "            {\n",
      "                \"name\": \"col 1\",\n",
      "                \"type\": \"string\"\n",
      "            },\n",
      "            {\n",
      "                \"name\": \"col 2\",\n",
      "                \"type\": \"string\"\n",
      "            }\n",
      "        ],\n",
      "        \"primaryKey\": [\n",
      "            \"index\"\n",
      "        ],\n",
      "        \"pandas_version\": \"1.4.0\"\n",
      "    },\n",
      "    \"data\": [\n",
      "        {\n",
      "            \"index\": \"row 1\",\n",
      "            \"col 1\": \"a\",\n",
      "            \"col 2\": \"b\"\n",
      "        },\n",
      "        {\n",
      "            \"index\": \"row 2\",\n",
      "            \"col 1\": \"c\",\n",
      "            \"col 2\": \"d\"\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "\u001b[1;31mFile:\u001b[0m      c:\\users\\dgadiraju\\projects\\python-revision-for-de\\pr-venv\\lib\\site-packages\\pandas\\core\\generic.py\n",
      "\u001b[1;31mType:\u001b[0m      method\n"
     ]
    }
   ],
   "source": [
    "orders.to_json?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stores using columnar format\n",
    "# Review data in the files to understand how data is formatted\n",
    "orders.to_json('data/retail_db/orders_json/part-00000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data using row format\n",
    "orders.to_json(\n",
    "    'data/retail_db/orders_json/part-00000',\n",
    "    orient='records',\n",
    "    lines=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>order_id</th>\n",
       "      <th>order_date</th>\n",
       "      <th>order_customer_id</th>\n",
       "      <th>order_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2013-07-25 00:00:00.0</td>\n",
       "      <td>11599</td>\n",
       "      <td>CLOSED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2013-07-25 00:00:00.0</td>\n",
       "      <td>256</td>\n",
       "      <td>PENDING_PAYMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2013-07-25 00:00:00.0</td>\n",
       "      <td>12111</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2013-07-25 00:00:00.0</td>\n",
       "      <td>8827</td>\n",
       "      <td>CLOSED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2013-07-25 00:00:00.0</td>\n",
       "      <td>11318</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68878</th>\n",
       "      <td>68879</td>\n",
       "      <td>2014-07-09 00:00:00.0</td>\n",
       "      <td>778</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68879</th>\n",
       "      <td>68880</td>\n",
       "      <td>2014-07-13 00:00:00.0</td>\n",
       "      <td>1117</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68880</th>\n",
       "      <td>68881</td>\n",
       "      <td>2014-07-19 00:00:00.0</td>\n",
       "      <td>2518</td>\n",
       "      <td>PENDING_PAYMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68881</th>\n",
       "      <td>68882</td>\n",
       "      <td>2014-07-22 00:00:00.0</td>\n",
       "      <td>10000</td>\n",
       "      <td>ON_HOLD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68882</th>\n",
       "      <td>68883</td>\n",
       "      <td>2014-07-23 00:00:00.0</td>\n",
       "      <td>5533</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>68883 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       order_id             order_date  order_customer_id     order_status\n",
       "0             1  2013-07-25 00:00:00.0              11599           CLOSED\n",
       "1             2  2013-07-25 00:00:00.0                256  PENDING_PAYMENT\n",
       "2             3  2013-07-25 00:00:00.0              12111         COMPLETE\n",
       "3             4  2013-07-25 00:00:00.0               8827           CLOSED\n",
       "4             5  2013-07-25 00:00:00.0              11318         COMPLETE\n",
       "...         ...                    ...                ...              ...\n",
       "68878     68879  2014-07-09 00:00:00.0                778         COMPLETE\n",
       "68879     68880  2014-07-13 00:00:00.0               1117         COMPLETE\n",
       "68880     68881  2014-07-19 00:00:00.0               2518  PENDING_PAYMENT\n",
       "68881     68882  2014-07-22 00:00:00.0              10000          ON_HOLD\n",
       "68882     68883  2014-07-23 00:00:00.0               5533         COMPLETE\n",
       "\n",
       "[68883 rows x 4 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_json(\n",
    "    'data/retail_db/orders_json/part-00000',\n",
    "    lines=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('pr-venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5421f294131e672cb561fd66dd4ddd0f901e75fd72a9cf1bf40a916143d2df64"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
